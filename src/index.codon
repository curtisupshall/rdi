# Seq imports
from bio import *

# Codon standard library imports
import sys
from threading import Lock
import openmp as omp
import datetime

# Python imports
from python import os, pickle, argparse, bz2, tqdm

# RDI imports
from archive import RdiArchive

INDEX_RDI_EXT = '.idx.rdi'

def _write_index(index, file_path):
    print(f'Writing index to disk... ')
    archive = RdiArchive[Dict[int, Dict[int, List[int]]]]()
    archive.write(index, file_path, INDEX_RDI_EXT)

def _load_index(file_path):
    print(f'Loading index from disk... ')
    archive = RdiArchive[Dict[int, Dict[int, List[int]]]]()
    return archive.load(file_path, INDEX_RDI_EXT)

# Generates a string repeat index, as well as a list of kmers
def _repeat_detection_index(
    n: int,
    SA: List[Int[32]],
    LCP: List[Int],
    k: int,
    report: Callable[[List[Tuple[int, int, int]]], None]
):
    CHUNK_SIZE = 2 ** 12

    print("Building RDI dictionary.")

    # Stores a list of kmers in a tuple. The first value of the tuple i is
    # the index of the kmer in the suffix array such that SA[i] yields the
    # position in the text that the kmer appears; The second value of the
    # tuple v is the number of occurrences
    kmer_counts = List[Tuple[int, int]]()

    dictionary = Dict[int, Dict[int, List[int]]]()
    progress = tqdm.tqdm(total=n, smoothing=0, colour='#1ED18B')

    # Found a kmer. Record the suffix array index i. From i, we can find all
    # positions in the text for this kmer using
    # `ps = [int(p) for p in SA[i:r]]`
    @omp.critical
    def report_kmer(i: int, r: int):
        return
        # TODO still need to decide how kmers are going to be stored. Ideally,
        # in a way which lets us easily associate them with reads
        
    @omp.critical
    def report_repeats(records: List[Tuple[int, int, int]]):
        
        # Call parent function
        report(records)
        progress.update(CHUNK_SIZE)
            
    # @par(schedule='dynamic', num_threads=2, ordered=True)
    for x in range(1, n + 1, CHUNK_SIZE):
        records = list()

        for i in range(x, min(x + CHUNK_SIZE - 1, n) + 1):
            if (LCP[i] < LCP[i + 1]):
                for k in range(LCP[i] + 1, LCP[i + 1] + 1):            
                    # Calculate j > i as the minimum value such that LCP[j] < LCP[i + 1]
                    j = i + 1
                    while j <= n + 1:
                        if (LCP[j] < k):
                            break
                        j += 1      

                    l: int = k                  # Length of the string
                    r: int = j - i              # Number of occurrences
                    
                    records.append((i, l, r))

        report_repeats(records)

    progress.close()
    # return (dictionary, kmer_counts)

# TODO
def repeat_detection_index(
    n: int,
    SA: List[Int[32]],
    LCP: List[Int],
    k: int,
    report: Callable[[List[Tuple[int, int, int]]], None],
    file_path: Optional[str]
):    
    if file_path is not None:
        try:
            # Check if RDI is already generated.
            index = _load_index(file_path)
            print("[debug] Loaded repeat detection index and kmer counts from file successfully")
            return index
        
        except:
            # Cannot read RDI from disk. Build the RDI.
            print("[debug] Failed to load repeat detection index.")

    index = _repeat_detection_index(n, SA, LCP, k, report)

    if file_path is not None:
        _write_index(index, file_path)
        
    return index
