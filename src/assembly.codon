from bio import *
from python import tqdm, json
import sys

import openmp as omp

from index import load_index
from jaccard import similarity
from archive import RdiArchive
from basecount import basepair_mode

# Deprecated 
# CANDIDATE_ANCHORS_RDI_EXT = '.cdt.rdi'
# ANCHOR_READS_RDI_EXT = '.anrd.rdi'
# CONTIGS_RDI_EXT = '.contigs.rdi'


# def cluster_reads(reads: List[seq]):
#     similarity_threshold = 0.0025

#     read_clusters: List[List[seq]] = List([List([reads[0]])])

#     for read in reads[1:]:
#         cluster_index = -1
#         greatest_similarity = -1.0
#         for index, cluster in enumerate(read_clusters):
#             similarity_index = similarity(read, cluster[0])
#             if similarity_index > similarity_threshold and similarity_index > greatest_similarity:
#                 cluster_index = index
#                 greatest_similarity = similarity_index
        
#         if cluster_index == -1:
#             read_clusters.append(List([read]))
#         else:
#             read_clusters[cluster_index].append(read)

#     return read_clusters


# # Generates a contig based on an anchor string and a list of reads which all contain the anchor string.
# def generate_contig(anchor, reads):
#     # print(f"Generating contig for {str(len(reads))} reads ...")
#     anchor_length = len(anchor)

#     contig_prefix_length = -1
#     contig_suffix_length = -1

#     # Step 1: Find the maximum prefix length before the anchor and the maximum suffix length after the
#     # anchor, along with their corresponding reads.
#     for read in reads:
#         start_index = str(read).index(anchor)
#         suffix_length = len(read) - (start_index + len(anchor))

#         if start_index > contig_prefix_length:
#             contig_prefix_length = start_index
            
#         if suffix_length > contig_suffix_length:
#             contig_suffix_length = suffix_length

#     # Allocate consensus array
#     consensus = [List[str]() for _ in range(contig_prefix_length + len(anchor) +  contig_suffix_length)]

#     for read in reads:
#         # For this read, determine how many base pairs appear before the anchor occurs
#         read_prefix_length = str(read).index(str(anchor))

#         # For this read, iterate over all the base pairs that appear before the anchor
#         relative_consensus_index = contig_prefix_length - read_prefix_length
#         for i in range(read_prefix_length):
#             consensus[relative_consensus_index + i].append(str(read)[i])

#         # Next, add the anchor to the consensus
#         for i in range(len(anchor)):
#             consensus[contig_prefix_length + i].append(anchor[i])
            
#         # Next, iterate over all the base pairs that appear after the anchor
#         read_suffix_start_index = read_prefix_length + anchor_length
#         read_suffix_length = len(read) - read_suffix_start_index
#         relative_consensus_index = contig_prefix_length + anchor_length
#         for i in range(read_suffix_length):
#             consensus[relative_consensus_index + i].append(str(read)[read_suffix_start_index + i])

#     contig = List[str]([])

#     for index, position_basepairs in enumerate(consensus):
#         (modal_base, popularity) = _get_basepair_statistics(position_basepairs)
#         if popularity < 0.5 or len(position_basepairs) < 10:
#             continue
        
#         contig.append(modal_base)
#         # print(f'{base} (position {index}, popularity: {str(popularity)} among {len(position_basepairs)} reads)')
#     return ''.join(contig)


