from bio import *
from python import tqdm, json
import sys

import openmp as omp

from index import load_index
from archive import RdiArchive
from basecount import basepair_mode

CANDIDATE_ANCHORS_RDI_EXT = '.cdt.rdi'
ANCHOR_READS_RDI_EXT = '.anrd.rdi'

def generate_anchor_queries(coverage: int, lmin: int, lmax: int, a: int) -> List[Tuple[int, int]]:

    ls = [l for l in range(lmin, lmax + 1)]
    cs = [c for c in range(coverage - a, coverage + a + 1)]

    return [(l, c) for l in ls for c in cs]

def find_candidate_anchors(Text: str, coverage: int, lmin: int, lmax: int, a: int, file_path: str):
    Rdi = load_index(file_path)

    print("Generating anchor queries...")
    queries = generate_anchor_queries(coverage, lmin, lmax, a)
    
    print("Querying index for cadidate...")
    anchors = List[Tuple[int, int, int]]()
    progress = tqdm.tqdm(total=len(queries), smoothing=0)

    # @omp.critical
    def report(p: int, l: int, r: int):
        anchors.append((p, l, r))

    # @omp.critical
    def increment_progress():
        progress.update(1)

    # @par(schedule='static', chunk_size=100, num_threads=32, ordered=False)
    for l, r in queries:
        # Find candidate strings using index
        rs = Rdi[l] if l in Rdi else {}
        ps = rs[r] if r in rs else List[int]([])
        for p in ps:
            substring = Text[p:p+l]
            if ('_' in substring):
                continue
            report(p, l, r)
        increment_progress()

    progress.close()
    return anchors


# Loads a set of candidate anchors from disk
def load_candidate_anchors(file_path) -> List[Tuple[int, int, int]]:
    archive = RdiArchive[List[Tuple[int, int, int]]]()
    return archive.load(file_path, CANDIDATE_ANCHORS_RDI_EXT)

# Loads anchor reads from disk
def load_anchor_reads(file_path) -> Dict[str, List[seq]]:
    archive = RdiArchive[Dict[str, List[seq]]]()
    return archive.load(file_path, ANCHOR_READS_RDI_EXT)

# Writes anchor reads to disk
def write_anchor_reads(anchor_reads, file_path):
    print('Saving anchor reads to disk...')
    archive = RdiArchive[Dict[str, List[seq]]]()
    archive.write(anchor_reads, file_path, ANCHOR_READS_RDI_EXT)

# Writes a set of candiate anchors to disk
def write_candidate_anchors(anchors, file_path):
    print('Saving candidate anchors to disk...')
    archive = RdiArchive[List[Tuple[int, int, int]]]()
    archive.write(anchors, file_path, CANDIDATE_ANCHORS_RDI_EXT)

def load_or_generate_candidate_anchors(Text: str, coverage: int, lmin: int, lmax: int, a: int, file_path: str, overwrite: bool = False):
    if not overwrite:
        try:
            # Check if candidate anchors are generated.
            anchors = load_candidate_anchors(file_path)
            print("[debug] Loaded candidate anchors from file successfully")
            return anchors

        except:
            # Cannot read anchors from disk. Build the anchor set.
            print("[debug] Failed to load candiate anchors.")

    anchors = find_candidate_anchors(Text, coverage, lmin, lmax, a, file_path)

    # Save the candidate anchors to disk
    write_candidate_anchors(anchors, file_path)

    return anchors

def find_or_load_reads_from_candidates(Text: str, anchors: List[Tuple[int, int, int]], file_path):
    try:
        # Check if candidate anchors are generated.
        anchor_reads = load_anchor_reads(file_path)
        print("[debug] Loaded anchors reads from file successfully")
        return anchor_reads

    except:
        # Cannot read anchors from disk. Build the anchor set.
        print("[debug] Failed to load anchor reads.")

    print(f"Collecting reads from {str(len(anchors))} anchors...")
    # TODO the anchor should be type seq, not str
    anchor_reads = Dict[str, List[seq]]({})

    for (p, l, r) in anchors:
        anchor_sequence = Text[p:p+l]
        anchor_reads[anchor_sequence] = List[seq]([])

    # Iterate over reads
    for read in FASTQ(file_path):
        for anchor_sequence in anchor_reads:
            if (anchor_sequence in str(read.seq).upper()):
                anchor_reads[anchor_sequence].append(read.seq)

    # avg = 0

    # for x in anchor_reads:
    #     avg += len(anchor_reads[x])
    #     print(f'{x}: {str(len(anchor_reads[x]))}')

    # print(f'Average read count: {str(avg / len(anchor_reads))}')
    
    write_anchor_reads(anchor_reads, file_path)

    return anchor_reads

# Generates a contig based on an anchor string and a list of reads which all contain the anchor string.
def generate_contig(anchor, reads):
    print(f"Generating contig for {anchor} ...")
    anchor_length = len(anchor)

    contig_prefix_length = -1
    contig_suffix_length = -1
    # max_prefix_read = ""
    # max_suffix_read = ""

    # Step 1: Find the maximum prefix length before the anchor and the maximum suffix length after the
    # anchor, along with their corresponding reads.
    for read in reads:
        start_index = str(read).index(anchor)
        suffix_length = len(read) - (start_index + len(anchor))

        if start_index > contig_prefix_length:
            contig_prefix_length = start_index
            # max_prefix_read = str(read)
            
        if suffix_length > contig_suffix_length:
            contig_suffix_length = suffix_length
            # max_suffix_read = str(read)

    # Allocate consensus array
    # consensus_length = contig_prefix_length + len(anchor) +  contig_suffix_length
    # consensus = __array__[List[int]](consensus_length)

    consensus = [List[str]() for _ in range(contig_prefix_length + len(anchor) +  contig_suffix_length)]

    for read in reads:
        # print(read)

        # For this read, determine how many base pairs appear before the anchor occurs
        read_prefix_length = str(read).index(str(anchor))

        # For this read, iterate over all the base pairs that appear before the anchor
        relative_consensus_index = contig_prefix_length - read_prefix_length
        for i in range(read_prefix_length):
            consensus[relative_consensus_index + i].append(str(read)[i])
            
        # Next, iterate over all the base pairs that appear after the anchor
        read_suffix_start_index = read_prefix_length + anchor_length
        read_suffix_length = len(read) - read_suffix_start_index
        relative_consensus_index = contig_prefix_length + anchor_length
        for i in range(read_suffix_length):
            consensus[relative_consensus_index + i].append(str(read)[read_suffix_start_index + i])
            
    print(consensus)


def generate_contigs(anchor, reads):
    print(f"Generating contigs for {anchor} ...")

    contigs = List[List[seq]]()

    prefix_consensus = Dict()
    suffix_consensus = Dict()

    num_reads = len(reads)
    consensus_min_size = 0.99 * num_reads

    # Iterate over all reads
    for read in reads:
        # Determine how many base pairs appear before the anchor occurs
        start_index = str(read).index(str(anchor))
        
        # Determine the index of the basepair immediately after the anchor
        end_index = start_index + len(anchor)
        
        # Iterate over all the base pairs that appear before the anchor
        for i in range(start_index):
            # Calculate index relative to anchor
            relative_index = i - start_index

            # Record consensus
            if relative_index not in prefix_consensus:
                prefix_consensus[relative_index] = List()                
            prefix_consensus[relative_index].append(read[i])
            
        # Iterate over all the base pairs that appear after the anchor
        for i in range(end_index, len(read)):
            # Record consensus

            relative_index = i - end_index # + 1 ???
            if relative_index not in suffix_consensus:
                suffix_consensus[relative_index] = List()                
            suffix_consensus[relative_index].append(read[i])

    # Construct consensus contig
    consensus_contig = str()
    
    # Prepend prefix
    for _, basepairs in sorted(prefix_consensus.items(), reverse=False):
        if len(basepairs) < consensus_min_size:
            # There is not enough coverage at this position in the contig to draw consensus.
            continue

        consensus_contig += basepair_mode(basepairs)
    
    # Append anchor
    consensus_contig += str(anchor)
    
    # Append suffix
    for _, basepairs in sorted(suffix_consensus.items(), reverse=False):
        if len(basepairs) < consensus_min_size:
            # There is not enough coverage at this position in the contig to draw consensus.
            continue

        consensus_contig += basepair_mode(basepairs)

    print('Consensus contig:')
    print(consensus_contig)
        

    return contigs
        
                

    
def perform_assembly(Text: str, file_path: str, coverage: int, lmin: int, lmax: int, a: int):
    anchors = load_or_generate_candidate_anchors(Text, coverage, lmin, lmax, a, file_path)
    anchor_reads = find_or_load_reads_from_candidates(Text, anchors, file_path)

    # progress = tqdm.tqdm(total=len(reads), smoothing=0)

    x = 0
    for anchor, reads in anchor_reads.items():

        generate_contig(str(anchor), reads)

        # Hack to only run one loop for testing purposes
        x += 1
        if (x > 0):
            return

    # contigs = bin_reads(anchor_reads, 10)
    # print(contigs)
    # for contig in contigs:
    #     for read in contig:
    #         semiglobal(str(read), str(contig[0]), True)
            
    #     break
